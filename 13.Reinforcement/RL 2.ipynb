{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Seminar_15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5d4e299a35694e3a9300d0c84538f1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8089f6876ed248cab140863c56f9651b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a2f82c333cd7401786792907fdf7f1a2",
              "IPY_MODEL_b66a71b128194ab19f10edcef461b4a1"
            ]
          }
        },
        "8089f6876ed248cab140863c56f9651b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a2f82c333cd7401786792907fdf7f1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1361eb37afe40ed931f0896e4d46170",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_75645aab92b54006a59754051d15ba5e"
          }
        },
        "b66a71b128194ab19f10edcef461b4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90751e0807cb4709a81747ccc973d59e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/? [06:43&lt;00:00,  4.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21f623d9da054330b247a195415ddde0"
          }
        },
        "f1361eb37afe40ed931f0896e4d46170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "75645aab92b54006a59754051d15ba5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90751e0807cb4709a81747ccc973d59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21f623d9da054330b247a195415ddde0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcclPSAKuK_u"
      },
      "source": [
        "## Семинар 15: \"Обучение с подкреплением 2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYllaCDeuK_v"
      },
      "source": [
        "ФИО: Калашников Дмитрий"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6p08QrRuK_w"
      },
      "source": [
        "###  FrozenLake\n",
        "\n",
        "\n",
        "<img src=\"http://vignette2.wikia.nocookie.net/riseoftheguardians/images/4/4c/Jack's_little_sister_on_the_ice.jpg/revision/latest?cb=20141218030206\" alt=\"a random image to attract attention\" style=\"width: 400px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBtCUYyHuK_x"
      },
      "source": [
        "import gym\n",
        "import numpy as np \n",
        "import random\n",
        "\n",
        "#create a single game instance\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "#start new game\n",
        "env.reset();"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnPgPH2puK_y",
        "outputId": "c4ac9a20-6baa-4928-fa47-5101252fe49d"
      },
      "source": [
        "# display the game state\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCyTMya1uK_z"
      },
      "source": [
        "### legend\n",
        "\n",
        "![img](https://cdn-images-1.medium.com/max/800/1*MCjDzR-wfMMkS0rPqXSmKw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS1P5DvxuK_0"
      },
      "source": [
        "## Задание 1.\n",
        "Подберите значения alpha и epsilon и найдите приближение оптимальной Q-функции для Frozen Lake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42fHVzJuK_0"
      },
      "source": [
        "class QLearn:\n",
        "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
        "        self.q = {}\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.actions = actions\n",
        "\n",
        "    def getQ(self, state, action):\n",
        "        return self.q.get((state, action), 0.0)\n",
        "\n",
        "    def learnQ(self, state, action, reward, value):\n",
        "        oldv = self.q.get((state, action), None)\n",
        "\n",
        "        if oldv is None:\n",
        "            self.q[(state, action)] = reward\n",
        "        else:\n",
        "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
        "\n",
        "    def chooseAction(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            action = random.choice(self.actions)\n",
        "        else:\n",
        "            q = [self.getQ(state, a) for a in self.actions]\n",
        "            maxQ = max(q)\n",
        "            count = q.count(maxQ)\n",
        "            if count > 1:\n",
        "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
        "                i = random.choice(best)\n",
        "            else:\n",
        "                i = q.index(maxQ)\n",
        "\n",
        "            action = self.actions[i]\n",
        "        return action\n",
        "\n",
        "    def learn(self, state1, action1, reward, state2):\n",
        "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
        "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXtzfJA9uK_2"
      },
      "source": [
        "def run_episode_qlearn_learn(env, qlearn, gamma = 1.0, render = False):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = qlearn.chooseAction(obs)\n",
        "        obs_new, reward, done, _ = env.step(action)\n",
        "        qlearn.learn(obs, action, reward, obs_new)\n",
        "        obs = obs_new\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxMCZ5GmuK_3"
      },
      "source": [
        "def run_episode_qlearn(env, qlearn, gamma = 1.0, render = False):\n",
        "    obs = env.reset()\n",
        "    total_reward = 0\n",
        "    step_idx = 0\n",
        "    while True:\n",
        "        if render:\n",
        "            env.render()\n",
        "        action = qlearn.chooseAction(obs)\n",
        "        obs_new, reward, done, _ = env.step(action)\n",
        "        obs = obs_new\n",
        "        total_reward += (gamma ** step_idx * reward)\n",
        "        step_idx += 1\n",
        "        if done:\n",
        "            break\n",
        "    return total_reward"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "y3HxtwocuK_5"
      },
      "source": [
        "def evaluate_qlearn(env, qlearn, gamma = 1.0, n = 100):\n",
        "    scores = [\n",
        "            run_episode_qlearn(env, qlearn, gamma = gamma, render = False)\n",
        "            for _ in range(n)]\n",
        "    return np.mean(scores)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62lBH1K_zOln"
      },
      "source": [
        "def qlearning(env, qlearn, gamma = 1.0, render = False, n_epochs=10, verbose=False):\n",
        "    for epoch in range(n_epochs):\n",
        "        total_reward = run_episode_qlearn(env, qlearn, gamma, render)\n",
        "        if verbose and epoch % 100 == 0:\n",
        "            print(f'reward {total_reward} for {epoch} epochs')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd_YFnMG1T8O"
      },
      "source": [
        "gamma = 1.0\n",
        "n_epochs = 1000\n",
        "epsilon = 0.21 # 0.1\n",
        "alpha = 0.89 # 0.2"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie6aTbAn3xlI"
      },
      "source": [
        "import itertools\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "5d4e299a35694e3a9300d0c84538f1a7",
            "8089f6876ed248cab140863c56f9651b",
            "a2f82c333cd7401786792907fdf7f1a2",
            "b66a71b128194ab19f10edcef461b4a1",
            "f1361eb37afe40ed931f0896e4d46170",
            "75645aab92b54006a59754051d15ba5e",
            "90751e0807cb4709a81747ccc973d59e",
            "21f623d9da054330b247a195415ddde0"
          ]
        },
        "id": "XO8JJLR-2SSF",
        "outputId": "a241c115-542b-4972-816c-7d68d520912d"
      },
      "source": [
        "results = []\n",
        "for gamma, epsilon, alpha in tqdm(itertools.product( np.linspace(0, 1.0, 5), np.linspace(0, 1.0, 20), np.linspace(0, 1.0, 20) )):\n",
        "    qlearn = QLearn(actions=range(env.env.nA), gamma=gamma, epsilon=epsilon, alpha=alpha)\n",
        "    env = gym.make(\"FrozenLake-v0\")\n",
        "    qlearning(env.env, qlearn, gamma, render=False, n_epochs=n_epochs)\n",
        "    q_score = evaluate_qlearn(env, qlearn, gamma = gamma, n = 100)\n",
        "    results.append([q_score, gamma, epsilon, alpha])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d4e299a35694e3a9300d0c84538f1a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXlmvRk4LJN",
        "outputId": "a8991c6f-0093-4e1f-fd95-379afe8d3255"
      },
      "source": [
        "sorted(results, reverse=True)[:15]"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.05, 1.0, 1.0, 0.10526315789473684],\n",
              " [0.05, 1.0, 0.8421052631578947, 0.21052631578947367],\n",
              " [0.04, 1.0, 0.9473684210526315, 0.5263157894736842],\n",
              " [0.04, 1.0, 0.894736842105263, 0.2631578947368421],\n",
              " [0.04, 1.0, 0.894736842105263, 0.05263157894736842],\n",
              " [0.04, 1.0, 0.8421052631578947, 0.7368421052631579],\n",
              " [0.04, 1.0, 0.7894736842105263, 0.7368421052631579],\n",
              " [0.04, 1.0, 0.7894736842105263, 0.631578947368421],\n",
              " [0.04, 1.0, 0.6842105263157894, 0.9473684210526315],\n",
              " [0.04, 1.0, 0.5263157894736842, 1.0],\n",
              " [0.04, 1.0, 0.47368421052631576, 0.5789473684210527],\n",
              " [0.04, 1.0, 0.42105263157894735, 0.9473684210526315],\n",
              " [0.04, 1.0, 0.3684210526315789, 0.3157894736842105],\n",
              " [0.04, 1.0, 0.21052631578947367, 0.894736842105263],\n",
              " [0.03, 1.0, 1.0, 0.47368421052631576]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thiwhhol6Ojx"
      },
      "source": [
        "gamma = 1.0\n",
        "n_epochs = 1000\n",
        "epsilon = 0.21\n",
        "alpha = 0.89"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYyfzTl16XaB",
        "outputId": "c0ece817-db85-4308-8f26-1e14795793d8"
      },
      "source": [
        "qlearn = QLearn(actions=range(env.env.nA), gamma=gamma, epsilon=epsilon, alpha=alpha)\n",
        "env = gym.make(\"FrozenLake-v0\")\n",
        "qlearning(env.env, qlearn, gamma, render=False, n_epochs=n_epochs)\n",
        "q_score = evaluate_qlearn(env, qlearn, gamma = gamma, n = 100)\n",
        "q_score"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "VEJu0iiVuK_7"
      },
      "source": [
        "## Задание 2.\n",
        "Обучите сеть DQN для среды http://gym.openai.com/envs/Pong-v0/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJgiyj-D81lZ",
        "outputId": "4e0e0813-c5ad-4f68-aac7-ae13f9d9ed44"
      },
      "source": [
        "pip install atari-py "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.7/dist-packages (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from atari-py) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfBZnlCx8_OB"
      },
      "source": [
        "import requests\n",
        "r = requests.get('http://www.atarimania.com/roms/Roms.rar')\n",
        "with open('Roms.rar', 'wb') as fout:\n",
        "    fout.write(r.content)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-4iKgFk9WzO"
      },
      "source": [
        "!unrar x Roms.rar\n",
        "!mkdir Roms\n",
        "!mv ROMS.zip \"HC ROMS.zip\" Roms\n",
        "!python -m atari_py.import_roms Roms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRu1NOTRYf2Z"
      },
      "source": [
        "### Обработка фреймов на уровне observation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSUUC5iHaHRn"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P2nuQJ660Qg"
      },
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    \"\"\" Wrapper for starting the game firstly \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99r6_spilDR7"
      },
      "source": [
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDWho7AlFo6"
      },
      "source": [
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "\n",
        "        return x_t.astype(np.uint8)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvhJMldCmgz-"
      },
      "source": [
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvFTGTPzmjI-"
      },
      "source": [
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nYFzAhJmlDe"
      },
      "source": [
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGuKWjnwmlVe"
      },
      "source": [
        "def create_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDuaoELUnMPu"
      },
      "source": [
        "### Сеть"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pkF4ydCZZKw"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=3),\n",
        "            nn.LayerNorm( (82, 82) ),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.LayerNorm( (80, 80) ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3),\n",
        "            nn.LayerNorm( (38, 38) ),\n",
        "            nn.ReLU(),\n",
        "            # nn.Conv2d(128, 256, kernel_size=3),\n",
        "            # nn.LayerNorm( (36, 36) ),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool2d(kernel_size=2),\n",
        "            # nn.Conv2d(256, 512, kernel_size=3),\n",
        "            # nn.LayerNorm( (16, 16) ),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        conv_out_size = 38 * 38 * 128 #  16 * 16 * 512 \n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 1024), # 512\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDRuj4X5nOef"
      },
      "source": [
        "### Классы для объектов из алгоритма"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOBbq-GEZhQA"
      },
      "source": [
        "MEAN_REWARD_BOUND = 15.0\n",
        "\n",
        "gamma = 0.95                \n",
        "batch_size = 64          \n",
        "replay_size = 10000            \n",
        "learning_rate = 1e-4      \n",
        "sync_target_frames = 10000\n",
        "replay_start_size = 5000      \n",
        "\n",
        "eps_start=1.0\n",
        "eps_decay=.9999\n",
        "eps_min=0.03"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9clnjFwvZjmD"
      },
      "source": [
        "class ExperienceReplay:\n",
        "    \"\"\" Experience Replay buffer for experience sampling from algorithm \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olZX8ldaZiGp"
      },
      "source": [
        "class Agent:\n",
        "    \"\"\" Agent from algorithm \"\"\"\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "\n",
        "        done_reward = None\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = (self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g969idRxFX9n"
      },
      "source": [
        "### Обучаем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4GAiOmfZoOr"
      },
      "source": [
        "def run():\n",
        "\n",
        "    DEFAULT_ENV_NAME = \"Pong-v0\"\n",
        "    env = create_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    buffer = ExperienceReplay(replay_size)\n",
        "    agent = Agent(env, buffer)\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    epsilon = eps_start\n",
        "    total_rewards = []\n",
        "    frame_idx = 0  \n",
        "\n",
        "    best_mean_reward = None\n",
        "    MAX_GAMES = 30\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon * eps_decay, eps_min)\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "\n",
        "        if reward is not None:\n",
        "\n",
        "            total_rewards.append(reward)\n",
        "            mean_reward = np.mean(total_rewards[-20:])\n",
        "\n",
        "            if len(total_rewards) % 1 == 0:\n",
        "                print(f\"game {len(total_rewards)} - reward {mean_reward}\")\n",
        "\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-final.pth\")\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    print(f\"Best mean reward updated {best_mean_reward}\")\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "        states_v = torch.tensor(states).to(device)\n",
        "        next_states_v = torch.tensor(next_states).to(device)\n",
        "        actions_v = torch.tensor(actions).to(device)\n",
        "        rewards_v = torch.tensor(rewards).to(device)\n",
        "        done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "        next_state_values = target_net(next_states_v).max(1)[0]\n",
        "        next_state_values[done_mask] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if frame_idx % sync_target_frames == 0:\n",
        "            target_net.load_state_dict(net.state_dict())\n",
        "        \n",
        "        if len(total_rewards) >= MAX_GAMES:\n",
        "            break\n",
        "    \n",
        "    if len(total_rewards) > MAX_GAMES:\n",
        "        print('Hasnt converged')\n",
        "    \n",
        "    return env, target_net"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srgqhAkla-fX",
        "outputId": "51c5c934-1549-483a-92ec-51a0421d04a2"
      },
      "source": [
        "env, target_net = run()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "game 1 - reward -21.0\n",
            "Best mean reward updated -21.0\n",
            "game 2 - reward -21.0\n",
            "game 3 - reward -21.0\n",
            "game 4 - reward -21.0\n",
            "game 5 - reward -20.8\n",
            "Best mean reward updated -20.8\n",
            "game 6 - reward -20.833333333333332\n",
            "game 7 - reward -20.428571428571427\n",
            "Best mean reward updated -20.428571428571427\n",
            "game 8 - reward -20.375\n",
            "Best mean reward updated -20.375\n",
            "game 9 - reward -20.444444444444443\n",
            "game 10 - reward -20.4\n",
            "game 11 - reward -20.363636363636363\n",
            "Best mean reward updated -20.363636363636363\n",
            "game 12 - reward -20.416666666666668\n",
            "game 13 - reward -20.46153846153846\n",
            "game 14 - reward -20.428571428571427\n",
            "game 15 - reward -20.466666666666665\n",
            "game 16 - reward -20.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "game 17 - reward -20.470588235294116\n",
            "game 18 - reward -20.444444444444443\n",
            "game 19 - reward -20.473684210526315\n",
            "game 20 - reward -20.4\n",
            "game 21 - reward -20.3\n",
            "Best mean reward updated -20.3\n",
            "game 22 - reward -20.25\n",
            "Best mean reward updated -20.25\n",
            "game 23 - reward -20.15\n",
            "Best mean reward updated -20.15\n",
            "game 24 - reward -20.15\n",
            "game 25 - reward -20.2\n",
            "game 26 - reward -20.1\n",
            "Best mean reward updated -20.1\n",
            "game 27 - reward -20.25\n",
            "game 28 - reward -20.3\n",
            "game 29 - reward -20.25\n",
            "game 30 - reward -20.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw8eBtOUFH6x"
      },
      "source": [
        "### Тестируем"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_o4FwM1zIIG"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "6FeEEaeyc2ax",
        "outputId": "99f03b13-8525-46e5-8aae-89b5f2c1e713"
      },
      "source": [
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "#net = target_net \n",
        "net.load_state_dict(torch.load('Pong-v0-final.pth', map_location=lambda storage, loc: storage))\n",
        "\n",
        "while True:\n",
        "\n",
        "    state_v = torch.tensor(np.array([state], copy=False))\n",
        "    q_vals = net(state_v.to('cuda')).data.cpu().numpy()[0]\n",
        "    action = np.argmax(q_vals)\n",
        "\n",
        "    clear_output(True)\n",
        "\n",
        "    screen = env.render(mode='rgb_array')\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    plt.figure(figsize=(5,8))\n",
        "    plt.imshow(screen.cpu().squeeze(0).numpy(), interpolation='none')\n",
        "    plt.show()\n",
        "\n",
        "    #time.sleep(0.01)\n",
        "\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Total reward: %.2f\" % total_reward)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward: -21.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmYZcO8QA1J-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}