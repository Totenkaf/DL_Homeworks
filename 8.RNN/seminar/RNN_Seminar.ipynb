{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Упражнение, для реализации \"Ванильной\" RNN\n",
    "* Попробуем обучить сеть восстанавливать слово hello по первой букве. т.е. построим charecter-level модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((3,3))*3\n",
    "b = torch.ones((3,3))*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45., 45., 45.],\n",
       "        [45., 45., 45.],\n",
       "        [45., 45., 45.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 15., 15.],\n",
       "        [15., 15., 15.],\n",
       "        [15., 15., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'ololoasdasddqweqw123456789'\n",
    "#word = 'hello'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет. \n",
    "Позволяет:\n",
    "* Закодировать символ при помощи one-hot\n",
    "* Делать итератор по слову, которыей возвращает текущий символ и следующий как таргет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataSet:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.chars2idx = {}\n",
    "        self.indexs  = []\n",
    "        for c in word: \n",
    "            if c not in self.chars2idx:\n",
    "                self.chars2idx[c] = len(self.chars2idx)\n",
    "                \n",
    "            self.indexs.append(self.chars2idx[c])\n",
    "            \n",
    "        self.vec_size = len(self.chars2idx)\n",
    "        self.seq_len  = len(word)\n",
    "        \n",
    "    def get_one_hot(self, idx):\n",
    "        x = torch.zeros(self.vec_size)\n",
    "        x[idx] = 1\n",
    "        return x\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.indexs[:-1], self.indexs[1:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq_len\n",
    "    \n",
    "    def get_char_by_id(self, id):\n",
    "        for c, i in self.chars2idx.items():\n",
    "            if id == i: return c\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация базовой RNN\n",
    "<br/>\n",
    "Скрытый элемент\n",
    "$$ h_t= tanh⁡ (W_{ℎℎ} h_{t−1}+W_{xh} x_t) $$\n",
    "Выход сети\n",
    "\n",
    "$$ y_t = W_{hy} h_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
    "        super(VanillaRNN, self).__init__()        \n",
    "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
    "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
    "        self.activation  = nn.Tanh()\n",
    "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
    "    \n",
    "    def forward(self, x, prev_hidden):\n",
    "        hidden = self.activation(self.x2hidden(x) + self.hidden(prev_hidden))\n",
    "#         Версия без активации - может происходить gradient exploding\n",
    "#         hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
    "        output = self.outweight(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация переменных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = WordDataSet(word=word)\n",
    "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=5, out_size=ds.vec_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "e_cnt     = 1000\n",
    "optim     = SGD(rnn.parameters(), lr = 0.05, momentum=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.98277282714844\n",
      "Clip gradient :  tensor(9.1748)\n",
      "63.775184631347656\n",
      "Clip gradient :  tensor(5.0961)\n",
      "48.03707504272461\n",
      "Clip gradient :  tensor(6.6118)\n",
      "34.6241569519043\n",
      "Clip gradient :  tensor(6.3688)\n",
      "24.486820220947266\n",
      "Clip gradient :  tensor(7.0706)\n",
      "18.007471084594727\n",
      "Clip gradient :  tensor(13.4131)\n",
      "14.059529304504395\n",
      "Clip gradient :  tensor(9.8575)\n",
      "11.126317024230957\n",
      "Clip gradient :  tensor(17.7017)\n",
      "9.300223350524902\n",
      "Clip gradient :  tensor(3.1053)\n",
      "7.5930585861206055\n",
      "Clip gradient :  tensor(3.0768)\n",
      "6.807809829711914\n",
      "Clip gradient :  tensor(3.7845)\n",
      "5.901915550231934\n",
      "Clip gradient :  tensor(3.2069)\n",
      "5.239877223968506\n",
      "Clip gradient :  tensor(3.7572)\n",
      "8.174659729003906\n",
      "Clip gradient :  tensor(56.1488)\n",
      "5.34053897857666\n",
      "Clip gradient :  tensor(4.6912)\n",
      "10.121846199035645\n",
      "Clip gradient :  tensor(72.4723)\n",
      "6.898985385894775\n",
      "Clip gradient :  tensor(11.5160)\n",
      "4.833205223083496\n",
      "Clip gradient :  tensor(3.1248)\n",
      "3.660797595977783\n",
      "Clip gradient :  tensor(6.1733)\n",
      "3.598662853240967\n",
      "Clip gradient :  tensor(5.6951)\n",
      "3.747034788131714\n",
      "Clip gradient :  tensor(7.8423)\n",
      "3.039869546890259\n",
      "Clip gradient :  tensor(3.1605)\n",
      "2.6833832263946533\n",
      "Clip gradient :  tensor(8.5800)\n",
      "4.144538402557373\n",
      "Clip gradient :  tensor(13.6879)\n",
      "2.310551643371582\n",
      "Clip gradient :  tensor(3.9107)\n",
      "2.1025454998016357\n",
      "Clip gradient :  tensor(9.3889)\n",
      "3.240795373916626\n",
      "Clip gradient :  tensor(11.2883)\n",
      "3.4507083892822266\n",
      "Clip gradient :  tensor(6.3788)\n",
      "2.5848922729492188\n",
      "Clip gradient :  tensor(10.3820)\n",
      "2.6497867107391357\n",
      "Clip gradient :  tensor(4.9159)\n",
      "2.2587597370147705\n",
      "Clip gradient :  tensor(5.5199)\n",
      "1.6189526319503784\n",
      "Clip gradient :  tensor(2.3423)\n",
      "1.1531751155853271\n",
      "Clip gradient :  tensor(0.9938)\n",
      "0.8754256963729858\n",
      "Clip gradient :  tensor(0.7320)\n",
      "0.7518529891967773\n",
      "Clip gradient :  tensor(0.5169)\n",
      "0.6595724821090698\n",
      "Clip gradient :  tensor(0.7295)\n",
      "0.748361349105835\n",
      "Clip gradient :  tensor(14.0629)\n",
      "1.154251217842102\n",
      "Clip gradient :  tensor(8.6016)\n",
      "0.9153725504875183\n",
      "Clip gradient :  tensor(2.2092)\n",
      "0.5974870324134827\n",
      "Clip gradient :  tensor(0.8750)\n",
      "0.4931560754776001\n",
      "Clip gradient :  tensor(0.4332)\n",
      "0.4249075651168823\n",
      "Clip gradient :  tensor(0.1581)\n",
      "0.3797775208950043\n",
      "Clip gradient :  tensor(0.1183)\n",
      "0.34519654512405396\n",
      "Clip gradient :  tensor(0.1065)\n",
      "0.31728124618530273\n",
      "Clip gradient :  tensor(0.0872)\n",
      "0.29397547245025635\n",
      "Clip gradient :  tensor(0.0796)\n",
      "0.27406132221221924\n",
      "Clip gradient :  tensor(0.0730)\n",
      "0.25677889585494995\n",
      "Clip gradient :  tensor(0.0683)\n",
      "0.24159970879554749\n",
      "Clip gradient :  tensor(0.0642)\n",
      "0.22814476490020752\n",
      "Clip gradient :  tensor(0.0606)\n",
      "0.216120645403862\n",
      "Clip gradient :  tensor(0.0575)\n",
      "0.20530545711517334\n",
      "Clip gradient :  tensor(0.0546)\n",
      "0.19552043080329895\n",
      "Clip gradient :  tensor(0.0521)\n",
      "0.18662157654762268\n",
      "Clip gradient :  tensor(0.0498)\n",
      "0.17849306762218475\n",
      "Clip gradient :  tensor(0.0477)\n",
      "0.17103596031665802\n",
      "Clip gradient :  tensor(0.0457)\n",
      "0.16417723894119263\n",
      "Clip gradient :  tensor(0.0565)\n",
      "0.15847666561603546\n",
      "Clip gradient :  tensor(0.2322)\n",
      "0.1520422399044037\n",
      "Clip gradient :  tensor(0.0991)\n",
      "0.14657750725746155\n",
      "Clip gradient :  tensor(0.1073)\n",
      "0.14140114188194275\n",
      "Clip gradient :  tensor(0.0430)\n",
      "0.13699862360954285\n",
      "Clip gradient :  tensor(0.2467)\n",
      "0.13231684267520905\n",
      "Clip gradient :  tensor(0.1439)\n",
      "0.12817907333374023\n",
      "Clip gradient :  tensor(0.1512)\n",
      "0.12449649721384048\n",
      "Clip gradient :  tensor(0.2423)\n",
      "0.1206207200884819\n",
      "Clip gradient :  tensor(0.2020)\n",
      "0.11750030517578125\n",
      "Clip gradient :  tensor(0.2876)\n",
      "0.11370468139648438\n",
      "Clip gradient :  tensor(0.1554)\n",
      "0.11050214618444443\n",
      "Clip gradient :  tensor(0.1164)\n",
      "0.10746979713439941\n",
      "Clip gradient :  tensor(0.0419)\n",
      "0.10466376692056656\n",
      "Clip gradient :  tensor(0.0791)\n",
      "0.10218678414821625\n",
      "Clip gradient :  tensor(0.1873)\n",
      "0.09956163167953491\n",
      "Clip gradient :  tensor(0.1596)\n",
      "0.09698052704334259\n",
      "Clip gradient :  tensor(0.0660)\n",
      "0.09509684145450592\n",
      "Clip gradient :  tensor(0.2421)\n",
      "0.09262004494667053\n",
      "Clip gradient :  tensor(0.1801)\n",
      "0.09029868245124817\n",
      "Clip gradient :  tensor(0.0392)\n",
      "0.08900084346532822\n",
      "Clip gradient :  tensor(0.3110)\n",
      "0.08644837141036987\n",
      "Clip gradient :  tensor(0.1326)\n",
      "0.08471234887838364\n",
      "Clip gradient :  tensor(0.1817)\n",
      "0.08269327878952026\n",
      "Clip gradient :  tensor(0.0666)\n",
      "0.08116260915994644\n",
      "Clip gradient :  tensor(0.1749)\n",
      "0.07934372872114182\n",
      "Clip gradient :  tensor(0.0850)\n",
      "0.07774566859006882\n",
      "Clip gradient :  tensor(0.0819)\n",
      "0.07618080824613571\n",
      "Clip gradient :  tensor(0.0393)\n",
      "0.07512179017066956\n",
      "Clip gradient :  tensor(0.2686)\n",
      "0.07350935786962509\n",
      "Clip gradient :  tensor(0.1584)\n",
      "0.07195650786161423\n",
      "Clip gradient :  tensor(0.0755)\n",
      "0.07059436291456223\n",
      "Clip gradient :  tensor(0.0282)\n",
      "0.06937053054571152\n",
      "Clip gradient :  tensor(0.0971)\n",
      "0.06827349960803986\n",
      "Clip gradient :  tensor(0.1706)\n",
      "0.06697461754083633\n",
      "Clip gradient :  tensor(0.1105)\n",
      "0.06576833128929138\n",
      "Clip gradient :  tensor(0.0783)\n",
      "0.06490601599216461\n",
      "Clip gradient :  tensor(0.2135)\n",
      "0.06369701772928238\n",
      "Clip gradient :  tensor(0.1484)\n",
      "0.06250833719968796\n",
      "Clip gradient :  tensor(0.0639)\n",
      "0.06184600293636322\n",
      "Clip gradient :  tensor(0.2465)\n",
      "0.06066354364156723\n",
      "Clip gradient :  tensor(0.1532)\n",
      "0.059776462614536285\n",
      "Clip gradient :  tensor(0.2009)\n",
      "0.05868404731154442\n",
      "Clip gradient :  tensor(0.1093)\n"
     ]
    }
   ],
   "source": [
    "CLIP_GRAD = True\n",
    "\n",
    "for epoch in range(e_cnt):\n",
    "    hh = torch.zeros(rnn.hidden.in_features)\n",
    "    loss = 0\n",
    "    optim.zero_grad()\n",
    "    for sample, next_sample in ds:\n",
    "        x = ds.get_one_hot(sample).unsqueeze(0)\n",
    "        target =  torch.LongTensor([next_sample])\n",
    "\n",
    "        y, hh = rnn(x, hh)\n",
    "        \n",
    "        loss += criterion(y, target)\n",
    "     \n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print (loss.data.item())\n",
    "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
    "    else: \n",
    "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
    "            \n",
    "#     print(\"Params : \")\n",
    "#     num_params = 0\n",
    "#     for item in rnn.parameters():\n",
    "#         num_params += 1\n",
    "#         print(item.grad)\n",
    "#     print(\"NumParams :\", num_params)\n",
    "#     print(\"Optimize\")\n",
    "    \n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t ololoasdasddqweqw123456789\n",
      "Original:\t ololoasdasddqweqw123456789\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "hh = torch.zeros(rnn.hidden.in_features)\n",
    "id = 0\n",
    "softmax  = nn.Softmax(dim=1)\n",
    "predword = ds.get_char_by_id(id)\n",
    "for c in enumerate(word[:-1]):\n",
    "    x = ds.get_one_hot(id).unsqueeze(0)\n",
    "    y, hh = rnn(x, hh)\n",
    "    y = softmax(y)\n",
    "    m, id = torch.max(y, 1)\n",
    "    id = id.data[0]\n",
    "    predword += ds.get_char_by_id(id)\n",
    "print ('Prediction:\\t' , predword)\n",
    "print(\"Original:\\t\", word)\n",
    "assert(predword == word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика\n",
    "Реализовать LSTM и GRU модули, обучить их предсказывать тестовое слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#тестовое слово\n",
    "word = 'ololoasdasddqweqw123456789'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию LSTM и обучить предсказывать слово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализовать GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Написать реализацию GRU и обучить предсказывать слово"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
